"""
Pipeline Principal - LAB03: Caracterizando a atividade de code review no GitHub

Este script integra todas as etapas do laborat√≥rio:
1. Coleta de dados do GitHub
2. An√°lise estat√≠stica
3. Gera√ß√£o de visualiza√ß√µes
4. Gera√ß√£o de relat√≥rio

Autor: [Seu Nome]
Data: [Data]
"""

import os
import sys
from datetime import datetime
import pandas as pd
import json

class LabPipeline:
    
    def __init__(self, github_token, output_dir='lab03_output'):
        self.token = github_token
        self.output_dir = output_dir
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        self.create_directories()
        
        print("=" * 80)
        print("LAB03 - Pipeline de An√°lise de Code Review no GitHub")
        print("=" * 80)
        print(f"Diret√≥rio de sa√≠da: {self.output_dir}")
        print(f"Timestamp: {self.timestamp}\n")
    
    def create_directories(self):
        dirs = [
            self.output_dir,
            f"{self.output_dir}/data",
            f"{self.output_dir}/plots",
            f"{self.output_dir}/reports"
        ]
        
        for dir_path in dirs:
            os.makedirs(dir_path, exist_ok=True)
    
    def step1_collect_repositories(self, limit=200, min_prs=100):

        print("\n" + "=" * 80)
        print("ETAPA 1: Coleta e Filtragem de Reposit√≥rios")
        print("=" * 80)
        
        from github_pr_collector import GitHubPRCollector
        
        collector = GitHubPRCollector(self.token)
        
        print(f"\n[1/2] Coletando top {limit} reposit√≥rios populares...")
        popular_repos = collector.get_popular_repositories(limit=limit)
        
        print(f"\n[2/2] Filtrando reposit√≥rios com pelo menos {min_prs} PRs...")
        filtered_repos = collector.filter_repositories(popular_repos, min_prs=min_prs)
        
        repos_file = f"{self.output_dir}/data/selected_repositories_{self.timestamp}.csv"
        repos_df = collector.save_repositories(filtered_repos, filename=repos_file)
        
        print(f"\n‚úì Etapa 1 conclu√≠da!")
        print(f"  - Total de reposit√≥rios selecionados: {len(filtered_repos)}")
        print(f"  - Arquivo salvo: {repos_file}")
        
        return filtered_repos, repos_df
    
    def step2_collect_prs(self, repositories, max_prs_per_repo=100, max_repos=None):

        print("\n" + "=" * 80)
        print("ETAPA 2: Coleta de Pull Requests e M√©tricas")
        print("=" * 80)
        
        from github_pr_collector import GitHubPRCollector
        
        collector = GitHubPRCollector(self.token)
        
        all_prs = []
        repos_to_process = repositories[:max_repos] if max_repos else repositories
        
        total_repos = len(repos_to_process)
        
        for idx, repo in enumerate(repos_to_process, 1):
            print(f"\n[{idx}/{total_repos}] Processando {repo['owner']}/{repo['name']}...")
            
            try:
                prs = collector.collect_prs_from_repo(
                    repo['owner'], 
                    repo['name'], 
                    max_prs=max_prs_per_repo
                )
                all_prs.extend(prs)
                print(f"  ‚úì Coletados {len(prs)} PRs")
            except Exception as e:
                print(f"  ‚úó Erro ao processar reposit√≥rio: {e}")
                continue
            
            if idx % 5 == 0:
                checkpoint_file = f"{self.output_dir}/data/checkpoint_{idx}_repos.csv"
                pd.DataFrame(all_prs).to_csv(checkpoint_file, index=False)
                print(f"  ‚Üí Checkpoint salvo: {checkpoint_file}")
        
        dataset_file = f"{self.output_dir}/data/github_prs_dataset_{self.timestamp}.csv"
        dataset_df = collector.save_dataset(all_prs, filename=dataset_file)
        
        print(f"\n‚úì Etapa 2 conclu√≠da!")
        print(f"  - Total de PRs coletados: {len(all_prs)}")
        print(f"  - Arquivo salvo: {dataset_file}")
        
        return dataset_df, dataset_file
    
    def step3_analyze_data(self, dataset_file):

        print("\n" + "=" * 80)
        print("ETAPA 3: An√°lise Estat√≠stica")
        print("=" * 80)
        
        from statistical_analysis import PRAnalyzer
        
        analyzer = PRAnalyzer(dataset_file)
        
        print("\nExecutando an√°lises para todas as RQs...")
        results = analyzer.run_all_analyses()
        
        results_file = f"{self.output_dir}/reports/analysis_results_{self.timestamp}.json"
        
        serializable_results = {}
        for rq, data in results.items():
            serializable_results[rq] = {}
            for key, value in data.items():
                if isinstance(value, pd.DataFrame):
                    serializable_results[rq][key] = value.to_dict()
                else:
                    serializable_results[rq][key] = value
        
        with open(results_file, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        
        report_file = f"{self.output_dir}/reports/analysis_results_{self.timestamp}.txt"
        analyzer.generate_report(output_file=report_file)
        
        print(f"\n‚úì Etapa 3 conclu√≠da!")
        print(f"  - Resultados JSON: {results_file}")
        print(f"  - Relat√≥rio textual: {report_file}")
        
        return analyzer, results
    
    def step4_generate_visualizations(self, dataset_file):

        print("\n" + "=" * 80)
        print("ETAPA 4: Gera√ß√£o de Visualiza√ß√µes")
        print("=" * 80)
        
        from data_visualization import PRVisualizer
        
        plots_dir = f"{self.output_dir}/plots/{self.timestamp}"
        os.makedirs(plots_dir, exist_ok=True)
        
        visualizer = PRVisualizer(dataset_file)
        
        print("\nGerando gr√°ficos...")
        
        visualizer.plot_status_distribution(f"{plots_dir}/01_status_distribution.png")
        visualizer.plot_size_comparison(f"{plots_dir}/02_size_comparison.png")
        visualizer.plot_time_analysis(f"{plots_dir}/03_time_analysis.png")
        visualizer.plot_description_analysis(f"{plots_dir}/04_description_analysis.png")
        visualizer.plot_interactions_analysis(f"{plots_dir}/05_interactions_analysis.png")
        visualizer.plot_reviews_vs_size(f"{plots_dir}/06_reviews_vs_size.png")
        visualizer.plot_reviews_vs_time(f"{plots_dir}/07_reviews_vs_time.png")
        visualizer.plot_reviews_vs_description(f"{plots_dir}/08_reviews_vs_description.png")
        visualizer.plot_reviews_vs_interactions(f"{plots_dir}/09_reviews_vs_interactions.png")
        visualizer.plot_correlation_heatmap(f"{plots_dir}/10_correlation_heatmap.png")
        
        print(f"\n‚úì Etapa 4 conclu√≠da!")
        print(f"  - Gr√°ficos salvos em: {plots_dir}")
        print(f"  - Total de visualiza√ß√µes: 10")
        
        return plots_dir
    
    def step5_generate_final_report(self, dataset_file, analyzer, results, plots_dir):
        """
        Etapa 5: Gerar relat√≥rio final em formato markdown
        Lab03S02 + Lab03S03: Relat√≥rio final (5 + 10 pontos)
        """
        print("\n" + "=" * 80)
        print("ETAPA 5: Gera√ß√£o do Relat√≥rio Final")
        print("=" * 80)
        
        df = pd.read_csv(dataset_file)
        
        report_file = f"{self.output_dir}/reports/relatorio_final_{self.timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            # Cabe√ßalho
            f.write("# Relat√≥rio Final - LAB03\n")
            f.write("## Caracterizando a atividade de code review no GitHub\n\n")
            f.write(f"**Data:** {datetime.now().strftime('%d/%m/%Y')}\n\n")
            f.write("---\n\n")
            
            # Estat√≠sticas gerais
            f.write("## 1. Vis√£o Geral do Dataset\n\n")
            f.write(f"- **Total de PRs analisados:** {len(df)}\n")
            f.write(f"- **PRs MERGED:** {sum(df['status'] == 'MERGED')} ({sum(df['status'] == 'MERGED')/len(df)*100:.1f}%)\n")
            f.write(f"- **PRs CLOSED:** {sum(df['status'] == 'CLOSED')} ({sum(df['status'] == 'CLOSED')/len(df)*100:.1f}%)\n\n")
            
            # Estat√≠sticas descritivas
            f.write("### Medianas das M√©tricas\n\n")
            f.write("| M√©trica | Geral | MERGED | CLOSED |\n")
            f.write("|---------|-------|--------|--------|\n")
            
            metrics = {
                'Arquivos Alterados': 'files_changed',
                'Linhas Adicionadas': 'additions',
                'Linhas Removidas': 'deletions',
                'Total de Linhas': 'total_lines_changed',
                'Tempo (horas)': 'time_to_close_hours',
                'Descri√ß√£o (caracteres)': 'body_length',
                'Participantes': 'num_participants',
                'Coment√°rios': 'num_comments',
                'Revis√µes': 'num_reviews'
            }
            
            for label, column in metrics.items():
                overall = df[column].median()
                merged = df[df['status'] == 'MERGED'][column].median()
                closed = df[df['status'] == 'CLOSED'][column].median()
                f.write(f"| {label} | {overall:.1f} | {merged:.1f} | {closed:.1f} |\n")
            
            f.write("\n---\n\n")
            
            # Resultados das RQs
            f.write("## 2. Resultados das Quest√µes de Pesquisa\n\n")
            
            # RQ01-RQ04 (Status)
            status_rqs = ['RQ01', 'RQ02', 'RQ03', 'RQ04']
            for rq in status_rqs:
                if rq in results:
                    f.write(f"### {rq}\n\n")
                    
                    if 'correlations' in results[rq]:
                        f.write("| M√©trica | Correla√ß√£o (œÅ) | p-value | Significante |\n")
                        f.write("|---------|---------------|---------|-------------|\n")
                        for var, corr in results[rq]['correlations'].items():
                            sig = "Sim ‚úì" if corr['significant'] else "N√£o ‚úó"
                            f.write(f"| {var} | {corr['correlation']:.4f} | {corr['p_value']:.6f} | {sig} |\n")
                    elif 'correlation' in results[rq]:
                        corr = results[rq]['correlation']
                        f.write(f"- **Correla√ß√£o (œÅ):** {corr['correlation']:.4f}\n")
                        f.write(f"- **p-value:** {corr['p_value']:.6f}\n")
                        f.write(f"- **Significante:** {'Sim ‚úì' if corr['significant'] else 'N√£o ‚úó'}\n")
                    
                    f.write("\n")
            
            f.write("---\n\n")
            
            # RQ05-RQ08 (Revis√µes)
            review_rqs = ['RQ05', 'RQ06', 'RQ07', 'RQ08']
            for rq in review_rqs:
                if rq in results:
                    f.write(f"### {rq}\n\n")
                    
                    if 'correlations' in results[rq]:
                        f.write("| M√©trica | Correla√ß√£o (œÅ) | p-value | Significante |\n")
                        f.write("|---------|---------------|---------|-------------|\n")
                        for var, corr in results[rq]['correlations'].items():
                            sig = "Sim ‚úì" if corr['significant'] else "N√£o ‚úó"
                            f.write(f"| {var} | {corr['correlation']:.4f} | {corr['p_value']:.6f} | {sig} |\n")
                    elif 'correlation' in results[rq]:
                        corr = results[rq]['correlation']
                        f.write(f"- **Correla√ß√£o (œÅ):** {corr['correlation']:.4f}\n")
                        f.write(f"- **p-value:** {corr['p_value']:.6f}\n")
                        f.write(f"- **Significante:** {'Sim ‚úì' if corr['significant'] else 'N√£o ‚úó'}\n")
                    
                    f.write("\n")
            
            f.write("---\n\n")
            
            f.write("## 3. Visualiza√ß√µes\n\n")
            f.write("As visualiza√ß√µes geradas encontram-se no diret√≥rio:\n")
            f.write(f"```\n{plots_dir}\n```\n\n")
            
            f.write("Lista de gr√°ficos:\n")
            plots = [
                "01_status_distribution.png - Distribui√ß√£o de Status",
                "02_size_comparison.png - RQ01: Tamanho vs Status",
                "03_time_analysis.png - RQ02: Tempo vs Status",
                "04_description_analysis.png - RQ03: Descri√ß√£o vs Status",
                "05_interactions_analysis.png - RQ04: Intera√ß√µes vs Status",
                "06_reviews_vs_size.png - RQ05: Tamanho vs Revis√µes",
                "07_reviews_vs_time.png - RQ06: Tempo vs Revis√µes",
                "08_reviews_vs_description.png - RQ07: Descri√ß√£o vs Revis√µes",
                "09_reviews_vs_interactions.png - RQ08: Intera√ß√µes vs Revis√µes",
                "10_correlation_heatmap.png - Matriz de Correla√ß√£o"
            ]
            
            for plot in plots:
                f.write(f"- `{plot}`\n")
            
            f.write("\n---\n\n")
            
            f.write("## 4. Conclus√£o\n\n")
            f.write("Este relat√≥rio apresenta os resultados da an√°lise de code review em reposit√≥rios populares do GitHub. ")
            f.write("Os dados coletados e analisados fornecem insights importantes sobre os fatores que influenciam ")
            f.write("o merge de Pull Requests e o n√∫mero de revis√µes necess√°rias.\n\n")
            f.write("Para uma an√°lise completa e discuss√£o detalhada dos resultados, consulte o template de relat√≥rio fornecido.\n\n")
        
        print(f"\n‚úì Etapa 5 conclu√≠da!")
        print(f"  - Relat√≥rio final salvo: {report_file}")
        
        return report_file
    
    def run_full_pipeline(self, limit_repos=200, min_prs=100, max_repos=10, max_prs_per_repo=100):
        """
        Executa o pipeline completo
        
        Par√¢metros:
        - limit_repos: N√∫mero de reposit√≥rios populares a buscar
        - min_prs: M√≠nimo de PRs que um reposit√≥rio deve ter
        - max_repos: M√°ximo de reposit√≥rios a processar (None = todos)
        - max_prs_per_repo: M√°ximo de PRs a coletar por reposit√≥rio
        """
        
        start_time = datetime.now()
        
        try:
            # Etapa 1: Coletar reposit√≥rios
            repositories, repos_df = self.step1_collect_repositories(
                limit=limit_repos, 
                min_prs=min_prs
            )
            
            # Etapa 2: Coletar PRs
            dataset_df, dataset_file = self.step2_collect_prs(
                repositories,
                max_prs_per_repo=max_prs_per_repo,
                max_repos=max_repos
            )
            
            # Etapa 3: An√°lise estat√≠stica
            analyzer, results = self.step3_analyze_data(dataset_file)
            
            # Etapa 4: Visualiza√ß√µes
            plots_dir = self.step4_generate_visualizations(dataset_file)
            
            # Etapa 5: Relat√≥rio final
            report_file = self.step5_generate_final_report(
                dataset_file, 
                analyzer, 
                results, 
                plots_dir
            )
            
            # Sum√°rio final
            end_time = datetime.now()
            duration = end_time - start_time
            
            print("\n" + "=" * 80)
            print("PIPELINE CONCLU√çDO COM SUCESSO!")
            print("=" * 80)
            print(f"\n‚è±Ô∏è  Tempo total de execu√ß√£o: {duration}")
            print(f"\nüìÅ Arquivos gerados:")
            print(f"   - Reposit√≥rios: {self.output_dir}/data/selected_repositories_{self.timestamp}.csv")
            print(f"   - Dataset: {dataset_file}")
            print(f"   - An√°lises: {self.output_dir}/reports/analysis_results_{self.timestamp}.txt")
            print(f"   - Visualiza√ß√µes: {plots_dir}")
            print(f"   - Relat√≥rio Final: {report_file}")
            
            print(f"\n‚úÖ Todos os arquivos foram salvos em: {self.output_dir}/")
            print("\n" + "=" * 80 + "\n")
            
            return {
                'repositories': repositories,
                'dataset_file': dataset_file,
                'results': results,
                'plots_dir': plots_dir,
                'report_file': report_file
            }
            
        except Exception as e:
            print(f"\n‚ùå ERRO no pipeline: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """Fun√ß√£o principal para execu√ß√£o do laborat√≥rio"""
    
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                              ‚ïë
‚ïë                    LAB03 - Code Review no GitHub                            ‚ïë
‚ïë                    Laborat√≥rio de Experimenta√ß√£o de Software                ‚ïë
‚ïë                                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    print("CONFIGURA√á√ÉO DO PIPELINE\n")
    print("Para executar este laborat√≥rio, voc√™ precisa:")
    print("1. Um token de acesso do GitHub")
    print("   ‚Üí Gere em: https://github.com/settings/tokens")
    print("   ‚Üí Permiss√µes necess√°rias: public_repo, read:user")
    print("\n2. As bibliotecas Python instaladas:")
    print("   ‚Üí pip install requests pandas scipy matplotlib seaborn numpy")
    print("\n" + "-" * 80 + "\n")
    
    github_token = input("Digite seu token do GitHub (ou pressione Enter para usar vari√°vel de ambiente): ").strip()
    
    if not github_token:
        github_token = os.environ.get('GITHUB_TOKEN')
        if not github_token:
            print("\n‚ùå Token n√£o fornecido. Configure a vari√°vel GITHUB_TOKEN ou forne√ßa o token.")
            return
    
    print("\n‚úì Token configurado!")
    
    print("\n" + "-" * 80)
    print("PAR√ÇMETROS DO PIPELINE")
    print("-" * 80 + "\n")
    
    print("Configura√ß√£o padr√£o (recomendada para testes):")
    print("  - Top 200 reposit√≥rios mais populares")
    print("  - M√≠nimo 100 PRs por reposit√≥rio")
    print("  - Processar 10 reposit√≥rios (para teste)")
    print("  - Coletar at√© 100 PRs por reposit√≥rio")
    print()
    
    use_default = input("Usar configura√ß√£o padr√£o? (s/n) [s]: ").strip().lower()
    
    if use_default == 'n':
        try:
            limit_repos = int(input("Limite de reposit√≥rios a buscar [200]: ") or "200")
            min_prs = int(input("M√≠nimo de PRs por reposit√≥rio [100]: ") or "100")
            max_repos = input("M√°ximo de reposit√≥rios a processar [10] (deixe vazio para todos): ").strip()
            max_repos = int(max_repos) if max_repos else None
            max_prs_per_repo = int(input("M√°ximo de PRs por reposit√≥rio [100]: ") or "100")
        except ValueError:
            print("‚ùå Valor inv√°lido. Usando configura√ß√£o padr√£o.")
            limit_repos, min_prs, max_repos, max_prs_per_repo = 200, 100, 10, 100
    else:
        limit_repos, min_prs, max_repos, max_prs_per_repo = 200, 100, 10, 100
    
    print("\n" + "=" * 80)
    print("INICIANDO PIPELINE")
    print("=" * 80)
    
    pipeline = LabPipeline(github_token=github_token)
    
    results = pipeline.run_full_pipeline(
        limit_repos=limit_repos,
        min_prs=min_prs,
        max_repos=max_repos,
        max_prs_per_repo=max_prs_per_repo
    )
    
    if results:
        print("\nüéâ Laborat√≥rio conclu√≠do com sucesso!")
        print("\nüìù Pr√≥ximos passos:")
        print("   1. Revise os resultados no diret√≥rio de sa√≠da")
        print("   2. Analise as visualiza√ß√µes geradas")
        print("   3. Complete o relat√≥rio final com suas an√°lises e discuss√µes")
        print("   4. Utilize o template fornecido para estruturar seu documento")
    else:
        print("\n‚ùå Ocorreu um erro durante a execu√ß√£o do pipeline.")
        print("   Verifique os logs acima para mais informa√ß√µes.")


if __name__ == "__main__":
    main()